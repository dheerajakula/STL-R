{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic\n",
      "3DBall\n",
      "3DBallHard\n",
      "GridWorld\n",
      "Hallway\n",
      "VisualHallway\n",
      "CrawlerDynamicTarget\n",
      "CrawlerStaticTarget\n",
      "Bouncer\n",
      "SoccerTwos\n",
      "PushBlock\n",
      "VisualPushBlock\n",
      "WallJump\n",
      "Tennis\n",
      "Reacher\n",
      "Pyramids\n",
      "VisualPyramids\n",
      "Walker\n",
      "FoodCollector\n",
      "VisualFoodCollector\n",
      "StrikersVsGoalie\n",
      "WormStaticTarget\n",
      "WormDynamicTarget\n"
     ]
    }
   ],
   "source": [
    "from mlagents_envs.registry import default_registry\n",
    "\n",
    "environment_names = list(default_registry.keys())\n",
    "for name in environment_names:\n",
    "   print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found path: /tmp/ml-agents-binaries/binaries/3DBall-b1b94a0ae13eef9d91f9d8db1e5770c5/Startup/Startup.x86_64\n"
     ]
    }
   ],
   "source": [
    "env = default_registry[\"3DBall\"].make()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Behavior Specs from the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of the behavior : 3DBall?team=0\n"
     ]
    }
   ],
   "source": [
    "# We will only consider the first Behavior\n",
    "behavior_name = list(env.behavior_specs)[0]\n",
    "print(f\"Name of the behavior : {behavior_name}\")\n",
    "spec = env.behavior_specs[behavior_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Observation Space from the Behavior Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations :  1\n",
      "Is there a visual observation ? False\n"
     ]
    }
   ],
   "source": [
    "# Examine the number of observations per Agent\n",
    "print(\"Number of observations : \", len(spec.observation_specs))\n",
    "\n",
    "# Is there a visual observation ?\n",
    "# Visual observation have 3 dimensions: Height, Width and number of channels\n",
    "vis_obs = any(len(spec.shape) == 3 for spec in spec.observation_specs)\n",
    "print(\"Is there a visual observation ?\", vis_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Action Space from the Behavior Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 continuous actions\n"
     ]
    }
   ],
   "source": [
    "# Is the Action continuous or multi-discrete ?\n",
    "if spec.action_spec.continuous_size > 0:\n",
    "  print(f\"There are {spec.action_spec.continuous_size} continuous actions\")\n",
    "if spec.action_spec.is_discrete():\n",
    "  print(f\"There are {spec.action_spec.discrete_size} discrete actions\")\n",
    "\n",
    "\n",
    "# How many actions are possible ?\n",
    "#print(f\"There are {spec.action_size} action(s)\")\n",
    "\n",
    "# For discrete actions only : How many different options does each action has ?\n",
    "if spec.action_spec.discrete_size > 0:\n",
    "  for action, branch_size in enumerate(spec.action_spec.discrete_branches):\n",
    "    print(f\"Action number {action} has {branch_size} different options\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the steps from the Environment\n",
    "\n",
    "You can do this with the env.get_steps(behavior_name) method. If there are multiple behaviors in the Environment, you can call this method with each of the behavior's names. Note This will not move the simulation forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_steps, terminal_steps = env.get_steps(behavior_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set actions for each behavior\n",
    "You can set the actions for the Agents of a Behavior by calling `env.set_actions()` you will need to specify the behavior name and pass a tensor of dimension 2. The first dimension of the action must be equal to the number of Agents that requested a decision during the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_actions(behavior_name, spec.action_spec.empty_action(len(decision_steps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Move the simulation forward\n",
    "Call `env.step()` to move the simulation forward. The simulation will progress until an Agent requestes a decision or terminates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decision_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mlagents_envs.base_env.ActionTuple object at 0x7f246306ecd0>\n"
     ]
    }
   ],
   "source": [
    "print(spec.action_spec.empty_action(len(decision_steps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the observations for one of the Agents\n",
    "`DecisionSteps.obs` is a tuple containing all of the observations for all of the Agents with the provided Behavior name.\n",
    "Each value in the tuple is an observation tensor containing the observation data for all of the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First vector observations :  [-0.01467304 -0.01468306 -0.5208206   4.         -0.79952097  0.\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "for index, obs_spec in enumerate(spec.observation_specs):\n",
    "  if len(obs_spec.shape) == 3:\n",
    "    print(\"Here is the first visual observation\")\n",
    "    plt.imshow(decision_steps.obs[index][0,:,:,:])\n",
    "    plt.show()\n",
    "\n",
    "for index, obs_spec in enumerate(spec.observation_specs):\n",
    "  if len(obs_spec.shape) == 1:\n",
    "    print(\"First vector observations : \", decision_steps.obs[index][0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Environment for a few episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Total rewards for episode 0 is 0.30000001937150955\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Total rewards for episode 1 is 0.700000025331974\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Total rewards for episode 2 is 1.1000000312924385\n"
     ]
    }
   ],
   "source": [
    "for episode in range(3):\n",
    "  env.reset()\n",
    "  decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "  tracked_agent = -1 # -1 indicates not yet tracking\n",
    "  done = False # For the tracked_agent\n",
    "  episode_rewards = 0 # For the tracked_agent\n",
    "  while not done:\n",
    "    # Track the first agent we see if not tracking\n",
    "    # Note : len(decision_steps) = [number of agents that requested a decision]\n",
    "    if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "      tracked_agent = decision_steps.agent_id[0]\n",
    "\n",
    "    # Generate an action for all agents\n",
    "    action = spec.action_spec.random_action(len(decision_steps))\n",
    "\n",
    "    # Set the actions\n",
    "    env.set_actions(behavior_name, action)\n",
    "\n",
    "    # Move the simulation forward\n",
    "    env.step()\n",
    "\n",
    "    # Get the new simulation results\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    if tracked_agent in decision_steps: # The agent requested a decision\n",
    "      print(tracked_agent)\n",
    "      episode_rewards += decision_steps[tracked_agent].reward\n",
    "    if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "      episode_rewards += terminal_steps[tracked_agent].reward\n",
    "      done = True\n",
    "  print(f\"Total rewards for episode {episode} is {episode_rewards}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from agent import Agent\n",
    "# import torch\n",
    "# agent = Agent(state_size=8, action_size=2, seed=0)\n",
    "# from collections import deque\n",
    "\n",
    "# def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "#     \"\"\"Deep Q-Learning.\n",
    "    \n",
    "#     Params\n",
    "#     ======\n",
    "#         n_episodes (int): maximum number of training episodes\n",
    "#         max_t (int): maximum number of timesteps per episode\n",
    "#         eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "#         eps_end (float): minimum value of epsilon\n",
    "#         eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "#     \"\"\"\n",
    "#     scores = []                        # list containing scores from each episode\n",
    "#     scores_window = deque(maxlen=100)  # last 100 scores\n",
    "#     eps = eps_start                    # initialize epsilon\n",
    "#     for i_episode in range(1, n_episodes+1):\n",
    "#         env_info = env.reset()\n",
    "#         state = env_info.vector_observations[0]\n",
    "#         score = 0 \n",
    "#         decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "#         # while not done:\n",
    "             \n",
    "#         #     if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "#         #         tracked_agent = decision_steps.agent_id[0]\n",
    "#         #     action = agent.act(state, eps).item()\n",
    "#         #     env.set_actions(behavior_name, action)\n",
    "#         #     env.step()\n",
    "\n",
    "#         #     # Get the new simulation results\n",
    "#         #     decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "#         #     if tracked_agent in decision_steps: # The agent requested a decision\n",
    "#         #         rewards += decision_steps[tracked_agent].reward\n",
    "#         #     if tracked_agent in terminal_steps: # The agent terminated its episode\n",
    "#         #         rewards += terminal_steps[tracked_agent].reward\n",
    "#         #         done = True\n",
    "\n",
    "#         for t in range(max_t):\n",
    "#             action = agent.act(state, eps).item()\n",
    "#             env_info = env.step(action)[0]\n",
    "#             next_state = env_info.vector_observations[0]   # get the next state\n",
    "#             reward = env_info.rewards[0]                   # get the reward\n",
    "#             done = env_info.local_done[0]                  # see if episode has finished\n",
    "#             agent.step(state, action, reward, next_state, done)\n",
    "#             score += reward                                # update the score\n",
    "#             state = next_state\n",
    "#             if done:\n",
    "#                 break \n",
    "#         scores_window.append(score)       # save most recent score\n",
    "#         scores.append(score)              # save most recent score\n",
    "#         eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "#         print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "#         if i_episode % 100 == 0:\n",
    "#             print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "#         if np.mean(scores_window)>=200.0:\n",
    "#             print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "#             torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "#             break\n",
    "#     return scores\n",
    "\n",
    "# scores = dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed environment\n"
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "print(\"Closed environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found path: /tmp/ml-agents-binaries/binaries/GridWorld-b1b94a0ae13eef9d91f9d8db1e5770c5/Startup/Startup.x86_64\n",
      "GridWorld environment created.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Buffer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_38627/3952075453.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mqnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisualQNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m84\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m126\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mexperiences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Buffer' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------\n",
    "# This code is used to close an env that might not have been closed before\n",
    "try:\n",
    "  env.close()\n",
    "except:\n",
    "  pass\n",
    "# -----------------\n",
    "\n",
    "from mlagents_envs.registry import default_registry\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create the GridWorld Environment from the registry\n",
    "env = default_registry[\"GridWorld\"].make()\n",
    "print(\"GridWorld environment created.\")\n",
    "\n",
    "from agent import *\n",
    "# Create a new Q-Network.\n",
    "qnet = VisualQNetwork((64, 84, 3), 126, 5)\n",
    "\n",
    "experiences: Buffer = []\n",
    "optim = torch.optim.Adam(qnet.parameters(), lr= 0.001)\n",
    "\n",
    "cumulative_rewards: List[float] = []\n",
    "\n",
    "# The number of training steps that will be performed\n",
    "NUM_TRAINING_STEPS = int(os.getenv('QLEARNING_NUM_TRAINING_STEPS', 70))\n",
    "# The number of experiences to collect per training step\n",
    "NUM_NEW_EXP = int(os.getenv('QLEARNING_NUM_NEW_EXP', 1000))\n",
    "# The maximum size of the Buffer\n",
    "BUFFER_SIZE = int(os.getenv('QLEARNING_BUFFER_SIZE', 10000))\n",
    "\n",
    "for n in range(NUM_TRAINING_STEPS):\n",
    "  new_exp,_ = Trainer.generate_trajectories(env, qnet, NUM_NEW_EXP, epsilon=0.1)\n",
    "  random.shuffle(experiences)\n",
    "  if len(experiences) > BUFFER_SIZE:\n",
    "    experiences = experiences[:BUFFER_SIZE]\n",
    "  experiences.extend(new_exp)\n",
    "  Trainer.update_q_net(qnet, optim, experiences, 5)\n",
    "  _, rewards = Trainer.generate_trajectories(env, qnet, 100, epsilon=0)\n",
    "  cumulative_rewards.append(rewards)\n",
    "  print(\"Training step \", n+1, \"\\treward \", rewards)\n",
    "\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Show the training graph\n",
    "plt.plot(range(NUM_TRAINING_STEPS), cumulative_rewards)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dcec866b866142c70c23a209f517d46e461c681246f4abe4420f9f2a46ea4ed9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('ml-agents': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
