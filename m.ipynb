{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbVXrmEsLXDt"
      },
      "source": [
        "# ML-Agents Q-Learning with GridWorld\n",
        "<img src=\"https://github.com/Unity-Technologies/ml-agents/blob/release_18_docs/docs/images/gridworld.png?raw=true\" align=\"middle\" width=\"435\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNKTwHU3d2-l"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzj7wgapAcDs"
      },
      "source": [
        "### Installing ml-agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N8yfQqkbebQ5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ml-agents already installed\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import mlagents\n",
        "  print(\"ml-agents already installed\")\n",
        "except ImportError:\n",
        "  !python -m pip install -q mlagents==0.27.0\n",
        "  print(\"Installed ml-agents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz81TWAkbuFY"
      },
      "source": [
        "## Train the GridWorld Environment with Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29n3dt1Zx5ty"
      },
      "source": [
        "### What is the GridWorld Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZhVRfdoyPmv"
      },
      "source": [
        "The [GridWorld](https://github.com/Unity-Technologies/ml-agents/blob/release_18_docs/docs/Learning-Environment-Examples.md#gridworld) Environment is a simple Unity visual environment. The Agent is a blue square in a 3x3 grid that is trying to reach a green __`+`__ while avoiding a red __`x`__.\n",
        "\n",
        "The observation is an image obtained by a camera on top of the grid.\n",
        "\n",
        "The Action can be one of 5 :\n",
        " - Do not move\n",
        " - Move up\n",
        " - Move down\n",
        " - Move right\n",
        " - Move left\n",
        "\n",
        "The Agent receives a reward of _1.0_ if it reaches the green __`+`__, a penalty of _-1.0_ if it touches the red __`x`__ and a penalty of `-0.01` at every step (to force the Agent to solve the task as fast as possible)\n",
        "\n",
        "__Note__ There are 9 Agents, each in their own grid, at once in the environment. This alows for faster data collection.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Gt-ZydJyJWD"
      },
      "source": [
        "### The Q-Learning Algorithm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP3DpfNL5bhM"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA1qOgfq0Xdv"
      },
      "source": [
        "In this Notebook, we will implement a very simple Q-Learning algorithm. We will use [pytorch](https://pytorch.org/) to do so.\n",
        "\n",
        "Below is the code to create the neural network we will use in the Notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q79rUp_Sx6A_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from typing import Tuple\n",
        "from math import floor\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VisualQNetwork(torch.nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    output_size: int\n",
        "  ):\n",
        "    \"\"\"\n",
        "    Creates a neural network that takes as input a batch of images (3\n",
        "    dimensional tensors) and outputs a batch of outputs (1 dimensional\n",
        "    tensors)\n",
        "    \"\"\"\n",
        "    super(VisualQNetwork, self).__init__()\n",
        "    fc1_units=16\n",
        "    fc2_units=32\n",
        "    seed = 1\n",
        "    self.seed = torch.manual_seed(seed)\n",
        "    \"*** YOUR CODE HERE ***\"\n",
        "    state_size = 8\n",
        "    self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "    self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "    self.fc3 = nn.Linear(fc2_units, output_size)\n",
        "\n",
        "  def forward(self, obs: torch.tensor):\n",
        "    \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "    x = F.relu(self.fc1(obs))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    return self.fc3(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZoaEBAo2L0F"
      },
      "source": [
        "We will now create a few classes to help us store the data we will use to train the Q-Learning algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "L772fe2q39DO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import NamedTuple, List\n",
        "\n",
        "\n",
        "class Experience(NamedTuple):\n",
        "  \"\"\"\n",
        "  An experience contains the data of one Agent transition.\n",
        "  - Observation\n",
        "  - Action\n",
        "  - Reward\n",
        "  - Done flag\n",
        "  - Next Observation\n",
        "  \"\"\"\n",
        "\n",
        "  obs: np.ndarray\n",
        "  action: np.ndarray\n",
        "  reward: float\n",
        "  done: bool\n",
        "  next_obs: np.ndarray\n",
        "\n",
        "# A Trajectory is an ordered sequence of Experiences\n",
        "Trajectory = List[Experience]\n",
        "\n",
        "# A Buffer is an unordered list of Experiences from multiple Trajectories\n",
        "Buffer = List[Experience]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HsM1d5I3_Tj"
      },
      "source": [
        "Now, we can create our trainer class. The role of this trainer is to collect data from the Environment according to a Policy, and then train the Q-Network with that data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KkzBoRJCb18t"
      },
      "outputs": [],
      "source": [
        "from mlagents_envs.environment import ActionTuple, BaseEnv\n",
        "from typing import Dict\n",
        "import random\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "  @staticmethod\n",
        "  def generate_trajectories(\n",
        "    env: BaseEnv, q_net: VisualQNetwork, buffer_size: int, epsilon: float\n",
        "  ):\n",
        "    \"\"\"\n",
        "    Given a Unity Environment and a Q-Network, this method will generate a\n",
        "    buffer of Experiences obtained by running the Environment with the Policy\n",
        "    derived from the Q-Network.\n",
        "    :param BaseEnv: The UnityEnvironment used.\n",
        "    :param q_net: The Q-Network used to collect the data.\n",
        "    :param buffer_size: The minimum size of the buffer this method will return.\n",
        "    :param epsilon: Will add a random normal variable with standard deviation.\n",
        "    epsilon to the value heads of the Q-Network to encourage exploration.\n",
        "    :returns: a Tuple containing the created buffer and the average cumulative\n",
        "    the Agents obtained.\n",
        "    \"\"\"\n",
        "    # Create an empty Buffer\n",
        "    buffer: Buffer = []\n",
        "\n",
        "    # Reset the environment\n",
        "    env.reset()\n",
        "    # Read and store the Behavior Name of the Environment\n",
        "    behavior_name = list(env.behavior_specs)[0]\n",
        "    # Read and store the Behavior Specs of the Environment\n",
        "    spec = env.behavior_specs[behavior_name]\n",
        "\n",
        "    # Create a Mapping from AgentId to Trajectories. This will help us create\n",
        "    # trajectories for each Agents\n",
        "    dict_trajectories_from_agent: Dict[int, Trajectory] = {}\n",
        "    # Create a Mapping from AgentId to the last observation of the Agent\n",
        "    dict_last_obs_from_agent: Dict[int, np.ndarray] = {}\n",
        "    # Create a Mapping from AgentId to the last observation of the Agent\n",
        "    dict_last_action_from_agent: Dict[int, np.ndarray] = {}\n",
        "    # Create a Mapping from AgentId to cumulative reward (Only for reporting)\n",
        "    dict_cumulative_reward_from_agent: Dict[int, float] = {}\n",
        "    # Create a list to store the cumulative rewards obtained so far\n",
        "    cumulative_rewards: List[float] = []\n",
        "\n",
        "    while len(buffer) < buffer_size:  # While not enough data in the buffer\n",
        "      # Get the Decision Steps and Terminal Steps of the Agents\n",
        "      decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
        "\n",
        "      # For all Agents with a Terminal Step:\n",
        "      for agent_id_terminated in terminal_steps:\n",
        "        # Create its last experience (is last because the Agent terminated)\n",
        "        last_experience = Experience(\n",
        "          obs=dict_last_obs_from_agent[agent_id_terminated].copy(),\n",
        "          reward=terminal_steps[agent_id_terminated].reward,\n",
        "          done=not terminal_steps[agent_id_terminated].interrupted,\n",
        "          action=dict_last_action_from_agent[agent_id_terminated].copy(),\n",
        "          next_obs=terminal_steps[agent_id_terminated].obs[0],\n",
        "        )\n",
        "        # Clear its last observation and action (Since the trajectory is over)\n",
        "        dict_last_obs_from_agent.pop(agent_id_terminated)\n",
        "        dict_last_action_from_agent.pop(agent_id_terminated)\n",
        "        # Report the cumulative reward\n",
        "        cumulative_reward = (\n",
        "          dict_cumulative_reward_from_agent.pop(agent_id_terminated)\n",
        "          + terminal_steps[agent_id_terminated].reward\n",
        "        )\n",
        "        cumulative_rewards.append(cumulative_reward)\n",
        "        # Add the Trajectory and the last experience to the buffer\n",
        "        buffer.extend(dict_trajectories_from_agent.pop(agent_id_terminated))\n",
        "        buffer.append(last_experience)\n",
        "\n",
        "      # For all Agents with a Decision Step:\n",
        "      for agent_id_decisions in decision_steps:\n",
        "        # If the Agent does not have a Trajectory, create an empty one\n",
        "        if agent_id_decisions not in dict_trajectories_from_agent:\n",
        "          dict_trajectories_from_agent[agent_id_decisions] = []\n",
        "          dict_cumulative_reward_from_agent[agent_id_decisions] = 0\n",
        "\n",
        "        # If the Agent requesting a decision has a \"last observation\"\n",
        "        if agent_id_decisions in dict_last_obs_from_agent:\n",
        "          # Create an Experience from the last observation and the Decision Step\n",
        "          exp = Experience(\n",
        "            obs=dict_last_obs_from_agent[agent_id_decisions].copy(),\n",
        "            reward=decision_steps[agent_id_decisions].reward,\n",
        "            done=False,\n",
        "            action=dict_last_action_from_agent[agent_id_decisions].copy(),\n",
        "            next_obs=decision_steps[agent_id_decisions].obs[0],\n",
        "          )\n",
        "          # Update the Trajectory of the Agent and its cumulative reward\n",
        "          dict_trajectories_from_agent[agent_id_decisions].append(exp)\n",
        "          dict_cumulative_reward_from_agent[agent_id_decisions] += (\n",
        "            decision_steps[agent_id_decisions].reward\n",
        "          )\n",
        "        # Store the observation as the new \"last observation\"\n",
        "        dict_last_obs_from_agent[agent_id_decisions] = (\n",
        "          decision_steps[agent_id_decisions].obs[0]\n",
        "        )\n",
        "\n",
        "      # Generate an action for all the Agents that requested a decision\n",
        "      # Compute the values for each action given the observation\n",
        "    \n",
        "     \n",
        "      # a = np.reshape(, (9, 64*84*3))\n",
        "      actions_values = (\n",
        "        \n",
        "        q_net(torch.from_numpy(decision_steps.obs[0])).detach().numpy()\n",
        "      )\n",
        "      \n",
        "      # Add some noise with epsilon to the values\n",
        "      actions_values += epsilon * (\n",
        "        np.random.randn(actions_values.shape[0], actions_values.shape[1])\n",
        "      ).astype(np.float32)\n",
        "\n",
        "      # Pick the best action using argmax\n",
        "      #actions = np.argmax(actions_values, axis=1)\n",
        "      #actions.resize((len(decision_steps), 2))\n",
        "      actions = actions_values\n",
        "      # Store the action that was picked, it will be put in the trajectory later\n",
        "      for agent_index, agent_id in enumerate(decision_steps.agent_id):\n",
        "        dict_last_action_from_agent[agent_id] = actions[agent_index]\n",
        "\n",
        "      # Set the actions in the environment\n",
        "      # Unity Environments expect ActionTuple instances.\n",
        "      action_tuple = ActionTuple()\n",
        "      action_tuple.add_continuous(actions)\n",
        "      #print(actions)\n",
        "      env.set_actions(behavior_name, action_tuple)\n",
        "      # Perform a step in the simulation\n",
        "      env.step()\n",
        "    return buffer, np.mean(cumulative_rewards)\n",
        "\n",
        "  @staticmethod\n",
        "  def update_q_net(\n",
        "    q_net: VisualQNetwork,\n",
        "    optimizer: torch.optim,\n",
        "    buffer: Buffer,\n",
        "    action_size: int\n",
        "  ):\n",
        "    \"\"\"\n",
        "    Performs an update of the Q-Network using the provided optimizer and buffer\n",
        "    \"\"\"\n",
        "    BATCH_SIZE = 128\n",
        "    NUM_EPOCH = 3\n",
        "    GAMMA = 0.9\n",
        "    batch_size = min(len(buffer), BATCH_SIZE)\n",
        "    random.shuffle(buffer)\n",
        "    # Split the buffer into batches\n",
        "    batches = [\n",
        "      buffer[batch_size * start : batch_size * (start + 1)]\n",
        "      for start in range(int(len(buffer) / batch_size))\n",
        "    ]\n",
        "    for _ in range(NUM_EPOCH):\n",
        "      for batch in batches:\n",
        "        # Create the Tensors that will be fed in the network\n",
        "        obs = torch.from_numpy(np.stack([ex.obs for ex in batch]))\n",
        "        reward = torch.from_numpy(\n",
        "          np.array([ex.reward for ex in batch], dtype=np.float32).reshape(-1, 1)\n",
        "        )\n",
        "        done = torch.from_numpy(\n",
        "          np.array([ex.done for ex in batch], dtype=np.float32).reshape(-1, 1)\n",
        "        )\n",
        "        action = torch.from_numpy(np.stack([ex.action for ex in batch]))\n",
        "        next_obs = torch.from_numpy(np.stack([ex.next_obs for ex in batch]))\n",
        "\n",
        "        # Use the Bellman equation to update the Q-Network\n",
        "        target = (\n",
        "          reward\n",
        "          + (1.0 - done)\n",
        "          * GAMMA\n",
        "          * torch.max(q_net(next_obs).detach(), dim=1, keepdim=True).values\n",
        "        )\n",
        "        #mask = torch.zeros((len(batch), action_size))\n",
        "        #mask.scatter_(1, action, 1)\n",
        "        #prediction = torch.sum(qnet(obs) * mask, dim=1, keepdim=True)\n",
        "        prediction = torch.sum(qnet(obs) , dim=1, keepdim=True)\n",
        "        criterion = torch.nn.MSELoss()\n",
        "        loss = criterion(prediction, target)\n",
        "\n",
        "        # Perform the backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcU4ZMAEWCvX"
      },
      "source": [
        "### Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_lIHijQfbYjh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found path: /tmp/ml-agents-binaries/binaries/3DBall-b1b94a0ae13eef9d91f9d8db1e5770c5/Startup/Startup.x86_64\n",
            "GridWorld environment created.\n",
            "Training step  1 \treward  0.5571428803460938\n",
            "Training step  2 \treward  0.6333333576718966\n",
            "Training step  3 \treward  0.7166666922469934\n",
            "Training step  4 \treward  0.5714285948446819\n",
            "Training step  5 \treward  0.40000002086162567\n",
            "Training step  6 \treward  0.8166666937371095\n",
            "Training step  7 \treward  0.6666666915019354\n",
            "Training step  8 \treward  0.7333333591620127\n",
            "Training step  9 \treward  0.5857143093432698\n",
            "Training step  10 \treward  0.8375000273808837\n",
            "Training step  11 \treward  0.6166666907568773\n",
            "Training step  12 \treward  0.6833333584169546\n",
            "Training step  13 \treward  0.5857143093432698\n",
            "Training step  14 \treward  0.5833333569268385\n",
            "Training step  15 \treward  0.7666666929920515\n",
            "Training step  16 \treward  0.6000000238418579\n",
            "Training step  17 \treward  0.650000024586916\n",
            "Training step  18 \treward  0.650000024586916\n",
            "Training step  19 \treward  0.7500000260770321\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_61380/169161347.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_TRAINING_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m   \u001b[0mnew_exp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_NEW_EXP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m   \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBUFFER_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_61380/3011713402.py\u001b[0m in \u001b[0;36mgenerate_trajectories\u001b[0;34m(env, q_net, buffer_size, epsilon)\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbehavior_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m       \u001b[0;31m# Perform a step in the simulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m       \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcumulative_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/ml-agents/lib/python3.7/site-packages/mlagents_envs/timers.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/ml-agents/lib/python3.7/site-packages/mlagents_envs/environment.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mstep_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"communicator.exchange\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexchange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUnityCommunicatorStoppedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Communicator has exited.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/ml-agents/lib/python3.7/site-packages/mlagents_envs/rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[0;34m(self, inputs, poll_callback)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll_for_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/ml-agents/lib/python3.7/site-packages/mlagents_envs/rpc_communicator.py\u001b[0m in \u001b[0;36mpoll_for_timeout\u001b[0;34m(self, poll_callback)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mcallback_timeout_wait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout_wait\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_timeout_wait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0;31m# Got an acknowledgment from the connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/ml-agents/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/ml-agents/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/ml-agents/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/ml-agents/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# -----------------\n",
        "# This code is used to close an env that might not have been closed before\n",
        "try:\n",
        "  env.close()\n",
        "except:\n",
        "  pass\n",
        "# -----------------\n",
        "\n",
        "from mlagents_envs.registry import default_registry\n",
        "from mlagents_envs.environment import UnityEnvironment\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Create the GridWorld Environment from the registry\n",
        "env = default_registry[\"3DBall\"].make()\n",
        "print(\"GridWorld environment created.\")\n",
        "\n",
        "# Create a new Q-Network.\n",
        "qnet = VisualQNetwork(2)\n",
        "\n",
        "experiences: Buffer = []\n",
        "optim = torch.optim.Adam(qnet.parameters(), lr= 0.001)\n",
        "\n",
        "cumulative_rewards: List[float] = []\n",
        "\n",
        "# The number of training steps that will be performed\n",
        "NUM_TRAINING_STEPS = int(os.getenv('QLEARNING_NUM_TRAINING_STEPS', 70))\n",
        "# The number of experiences to collect per training step\n",
        "NUM_NEW_EXP = int(os.getenv('QLEARNING_NUM_NEW_EXP', 100))\n",
        "# The maximum size of the Buffer\n",
        "BUFFER_SIZE = int(os.getenv('QLEARNING_BUFFER_SIZE', 10000))\n",
        "\n",
        "for n in range(NUM_TRAINING_STEPS):\n",
        "  new_exp,_ = Trainer.generate_trajectories(env, qnet, NUM_NEW_EXP, epsilon=0.9)\n",
        "  random.shuffle(experiences)\n",
        "  if len(experiences) > BUFFER_SIZE:\n",
        "    experiences = experiences[:BUFFER_SIZE]\n",
        "  experiences.extend(new_exp)\n",
        "  Trainer.update_q_net(qnet, optim, experiences, 2)\n",
        "  _, rewards = Trainer.generate_trajectories(env, qnet, 100, epsilon=0)\n",
        "  cumulative_rewards.append(rewards)\n",
        "  print(\"Training step \", n+1, \"\\treward \", rewards)\n",
        "\n",
        "\n",
        "env.close()\n",
        "\n",
        "# Show the training graph\n",
        "plt.plot(range(NUM_TRAINING_STEPS), cumulative_rewards)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ihb_gmYLUsH"
      },
      "outputs": [
        {
          "ename": "UnityEnvironmentException",
          "evalue": "No Unity environment is loaded.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnityEnvironmentException\u001b[0m                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_56410/3751641700.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/anaconda3/envs/ml-agents/lib/python3.7/site-packages/mlagents_envs/environment.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mUnityEnvironmentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No Unity environment is loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnityEnvironmentException\u001b[0m: No Unity environment is loaded."
          ]
        }
      ],
      "source": [
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Colab-UnityEnvironment-2-Train.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
